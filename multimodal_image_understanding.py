# -*- coding: utf-8 -*-
"""multimodal_image_understanding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eM3n2it09hHB3x9_Ez9DZ6SojA-BFiee

# LLaVA Vision-Language Model - 5 Practical Uses Demo

This notebook demonstrates the 5 practical uses of LLaVA (llava-hf/llava-1.5-7b-hf):
1. Image Captioning
2. Tag/Object/Concept Extraction
3. Instruction Following with Images
4. OCR-like Understanding (Text in Images)
5. Structured Output Generation

**Libraries used:**
- transformers: For LLaVA model and processor
- torch: PyTorch backend
- PIL: Image loading and processing
"""

# -*- coding: utf-8 -*-
"""
LLaVA FastAPI Server for Google Colab
This server runs the LLaVA model and exposes it via FastAPI with ngrok tunneling
"""

# Install required packages
print("Installing required packages...")
import subprocess
import sys

packages = [
    "fastapi",
    "uvicorn",
    "pyngrok",
    "transformers",
    "torch",
    "pillow",
    "accelerate",
]

for package in packages:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

print("‚úì All packages installed successfully!")

# Import libraries
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
import torch
import torch.nn.functional as F
from transformers import AutoProcessor, LlavaForConditionalGeneration, CLIPModel, CLIPProcessor
from PIL import Image
import io
import json
from pyngrok import ngrok
import uvicorn
from typing import Optional
import threading

print("‚úì Libraries imported successfully!")

# Initialize FastAPI app
app = FastAPI(
    title="LLaVA Vision-Language Model API",
    description="API for image analysis using LLaVA model",
    version="1.0.0"
)

# Global variables for LLaVA model and processor
processor = None
model = None
model_loaded = False
device = None
dtype = None

# Global variables for CLIP model and processor
clip_model = None
clip_processor = None
clip_model_loaded = False

def load_llava_model():
    """
    Load the LLaVA model and processor from Hugging Face
    Model: llava-hf/llava-1.5-7b-hf
    """
    global processor, model, model_loaded, device, dtype

    if model_loaded:
        print("Model already loaded!")
        return

    print("=" * 70)
    print("Loading LLaVA model and processor...")
    print("This may take 2-5 minutes on first run...")
    print("=" * 70)

    if torch.cuda.is_available():
        device = torch.device("cuda")
        dtype = torch.float16
        print("‚úì CUDA available - using GPU")
        print(f"‚úì GPU: {torch.cuda.get_device_name(0)}")

    else:
        device = torch.device("cpu")
        dtype = torch.float32
        print("‚ö†Ô∏è  CUDA not available - using CPU")
        print("‚ö†Ô∏è  Note: CPU inference will be slower")

    model_id = "llava-hf/llava-1.5-7b-hf"

    # Load processor
    processor = AutoProcessor.from_pretrained(model_id)

    # Load model with appropriate settings for CPU or GPU
    if device.type == "cuda":
        model = LlavaForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=dtype,
            device_map="auto"
        )
    else:
        # For CPU, load without device_map and move manually
        model = LlavaForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=dtype,
            low_cpu_mem_usage=True
        )
        model = model.to(device)

    model_loaded = True

    print("‚úì Model loaded successfully!")
    print(f"‚úì Model device: {model.device}")
    print(f"‚úì Model dtype: {model.dtype}")
    print("=" * 70)

def load_clip_model():
    """
    Load the CLIP model (ViT-B/32)
    This model acts as the OpenVision encoder
    It converts images into 512-dimensional embeddings
    """
    global clip_model, clip_processor, clip_model_loaded

    if clip_model_loaded:
        print("CLIP model already loaded!")
        return

    print("=" * 70)
    print("Loading CLIP model and processor...")
    print("=" * 70)

    MODEL_NAME = "openai/clip-vit-base-patch32"

    clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)
    clip_model = CLIPModel.from_pretrained(MODEL_NAME)

    # Move to GPU if available
    if torch.cuda.is_available():
        clip_model = clip_model.to("cuda")
        print("‚úì CLIP model loaded on GPU")
    else:
        print("‚úì CLIP model loaded on CPU")

    # Set model to evaluation mode (important: no training)
    clip_model.eval()

    clip_model_loaded = True
    print("‚úì CLIP model loaded successfully")
    print("=" * 70)

def generate_embeddings(image: Image.Image):
    """
    Preprocess image and convert it to model input
    Generate normalized embeddings for the image
    
    Args:
        image: PIL Image object
    
    Returns:
        Normalized embedding tensor of shape [1, 512]
    """
    if not clip_model_loaded:
        raise HTTPException(status_code=503, detail="CLIP model not loaded yet")

    # Preprocess image
    inputs = clip_processor(images=image, return_tensors="pt")
    
    # Move to same device as model
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    # Disable gradient calculation (we are not training)
    with torch.no_grad():
        image_features = clip_model.get_image_features(**inputs)

    # Normalize embeddings to unit length (L2 normalization)
    # This is required for cosine similarity to work correctly
    image_embedding = F.normalize(image_features, p=2, dim=1)
    
    return image_embedding

def cosine_similarity(embedding1, embedding2):
    # Calculate cosine similarity between two embeddings
    # Cosine similarity module
    cosine = torch.nn.CosineSimilarity(dim=1)

    # Compare image1 with itself
    similarity_same = cosine(image1_embedding, image1_embedding)

    print("Cosine similarity (same image):", similarity_same.item())
    # Compare image1 with image2
    similarity_diff = cosine(image1_embedding, image2_embedding)

def generate_response(image: Image.Image, prompt: str) -> str:
    """
    Generate a response from LLaVA given an image and text prompt

    Args:
        image: PIL Image object
        prompt: Text prompt/instruction

    Returns:
        Generated text response
    """
    if not model_loaded:
        raise HTTPException(status_code=503, detail="Model not loaded yet")

    # Prepare conversation format (LLaVA uses a specific format)
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": prompt},
            ],
        },
    ]

    # Apply chat template
    prompt_text = processor.apply_chat_template(conversation, add_generation_prompt=True)

    # Process inputs
    inputs = processor(images=image, text=prompt_text, return_tensors="pt").to(0, torch.float16)

    # Generate response
    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)

    # Decode and extract only the generated text (remove prompt)
    generated_text = processor.decode(output[0], skip_special_tokens=True)

    # Extract only the assistant's response
    if "ASSISTANT:" in generated_text:
        response = generated_text.split("ASSISTANT:")[-1].strip()
    else:
        response = generated_text.strip()

    return response

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "LLaVA Vision-Language Model API",
        "status": "running",
        "model_loaded": model_loaded,
        "endpoints": {
            "analyze": "/v1/analyze",
            "embed": "/v1/embed",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": model_loaded,
        "cuda_available": torch.cuda.is_available(),
        "device": str(model.device) if model_loaded else "N/A"
    }

@app.post("/v1/analyze")
async def analyze_image(
    file: UploadFile = File(...),
    prompt: str = Form(...)
):
    """
    Analyze an uploaded image using LLaVA model

    Args:
        file: Image file to analyze
        prompt: Text prompt/instruction for the model

    Returns:
        JSON response with analysis results
    """
    try:
        # Validate file type
        if not file.content_type.startswith("image/"):
            raise HTTPException(status_code=400, detail="File must be an image")

        # Read and process image
        image_bytes = await file.read()
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        print(f"Processing image: {file.filename}")
        print(f"Image size: {image.size}")
        print(f"Prompt: {prompt}")

        # Generate response using LLaVA
        response = generate_response(image, prompt)

        print(f"Response: {response}")

        return {
            "success": True,
            "filename": file.filename,
            "image_size": {
                "width": image.size[0],
                "height": image.size[1]
            },
            "prompt": prompt,
            "response": response
        }

    except Exception as e:
        print(f"Error processing image: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing image: {str(e)}")

@app.post("/v1/embed")
async def embed_image(
    file: UploadFile = File(...)
):
    """
    Generate embeddings for an uploaded image using CLIP model
    
    Args:
        file: Image file to generate embeddings for
    
    Returns:
        JSON response with embedding vector (512-dimensional)
    """
    try:
        # Validate file type
        if not file.content_type.startswith("image/"):
            raise HTTPException(status_code=400, detail="File must be an image")

        # Read and process image
        image_bytes = await file.read()
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        print(f"Generating embeddings for: {file.filename}")
        print(f"Image size: {image.size}")

        # Generate embeddings using CLIP
        embedding = generate_embeddings(image)
        
        # Convert to list for JSON serialization
        embedding_list = embedding.cpu().numpy().tolist()[0]

        print(f"Embedding shape: {embedding.shape}")
        print(f"Embedding generated successfully")

        return {
            "success": True,
            "filename": file.filename,
            "image_size": {
                "width": image.size[0],
                "height": image.size[1]
            },
            "embedding": embedding_list,
            "embedding_shape": list(embedding.shape),
            "embedding_dim": embedding.shape[1]
        }

    except Exception as e:
        print(f"Error generating embeddings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating embeddings: {str(e)}")


# Setup ngrok tunnel and run server
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("STARTING LLAVA FASTAPI SERVER")
    print("=" * 70)

    # Load models before starting server
    load_llava_model()
    load_clip_model()

    # Set ngrok auth token (you'll need to add your token)
    # Get free token from: https://dashboard.ngrok.com/get-started/your-authtoken
    # Uncomment and add your token:
    ngrok.set_auth_token("37lUpwcYVFfOuHkVQBCfhfZ66OE_7uSiArpZk7HsmLcHX3Wqd")

    # Start ngrok tunnel
    print("\nStarting ngrok tunnel...")
    public_url = ngrok.connect(8000)
    print(f"\n{'=' * 70}")
    print(f"üåê PUBLIC URL: {public_url}")
    print(f"{'=' * 70}")
    print(f"\n‚ö†Ô∏è  IMPORTANT: Copy this URL and use it in your local app.py")
    print(f"   Update COLAB_SERVER_URL = '{public_url}'\n")
    print(f"{"=" * 70}\n")

    # Run FastAPI server using threading to avoid event loop issues
    config = uvicorn.Config(app, host="127.0.0.1", port=8000, log_level="info")
    server = uvicorn.Server(config)

    # Run server in a thread to work in Colab
    thread = threading.Thread(target=server.run)
    thread.start()

    print("\n‚úì Server is running!")
    print("‚úì Keep this cell running - do NOT stop it")
    print("‚úì Use the public URL above in your local application\n")

    # Keep the main thread alive
    thread.join()

