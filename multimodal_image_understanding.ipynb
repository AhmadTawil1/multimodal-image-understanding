{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9719c562f4343808878eb70a742fd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92ec6fa254b0451f98dafdc46d89f61b",
              "IPY_MODEL_013cef8ecfb0420397c92b591ab24e4e",
              "IPY_MODEL_945d21b9aadc4e67855e6404fd3619d4"
            ],
            "layout": "IPY_MODEL_c486bbf31bdd4a798e7522d56613dcca"
          }
        },
        "92ec6fa254b0451f98dafdc46d89f61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803e045002e84481a3998adb95867b4b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_96834b2eb7b74e59a9ed1144f9f7d9ab",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "013cef8ecfb0420397c92b591ab24e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c65f3e2924e4163b51fe8020335a8e8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2804ad5cafa9463ab34d56e12597a304",
            "value": 3
          }
        },
        "945d21b9aadc4e67855e6404fd3619d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d920393404488b83f06b07a0765a44",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1dcd16bab8a249d88db2fea0bc81926a",
            "value": "‚Äá3/3‚Äá[01:07&lt;00:00,‚Äá21.62s/it]"
          }
        },
        "c486bbf31bdd4a798e7522d56613dcca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "803e045002e84481a3998adb95867b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96834b2eb7b74e59a9ed1144f9f7d9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c65f3e2924e4163b51fe8020335a8e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2804ad5cafa9463ab34d56e12597a304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08d920393404488b83f06b07a0765a44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dcd16bab8a249d88db2fea0bc81926a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLaVA Vision-Language Model - 5 Practical Uses Demo\n",
        "\n",
        "This notebook demonstrates the 5 practical uses of LLaVA (llava-hf/llava-1.5-7b-hf):\n",
        "1. Image Captioning\n",
        "2. Tag/Object/Concept Extraction\n",
        "3. Instruction Following with Images\n",
        "4. OCR-like Understanding (Text in Images)\n",
        "5. Structured Output Generation\n",
        "\n",
        "**Libraries used:**\n",
        "- transformers: For LLaVA model and processor\n",
        "- torch: PyTorch backend\n",
        "- PIL: Image loading and processing\n",
        "\n"
      ],
      "metadata": {
        "id": "SFexej7NSJKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LLaVA FastAPI Server for Google Colab\n",
        "This server runs the LLaVA model and exposes it via FastAPI with ngrok tunneling\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "print(\"Installing required packages...\")\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = [\n",
        "    \"fastapi\",\n",
        "    \"uvicorn\",\n",
        "    \"pyngrok\",\n",
        "    \"transformers\",\n",
        "    \"torch\",\n",
        "    \"pillow\",\n",
        "    \"accelerate\",\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "print(\"‚úì All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLvdCeXC2e3w",
        "outputId": "549ef076-2366-4a46-f725-80afcb8ea1ce",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "‚úì All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from typing import Optional\n",
        "import threading\n",
        "\n",
        "print(\"‚úì Libraries imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRUZdLuA6Hsm",
        "outputId": "bfce90ad-6d97-4965-951d-1087900f8643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"LLaVA Vision-Language Model API\",\n",
        "    description=\"API for image analysis using LLaVA model\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Global variables for LLaVA model and processor\n",
        "processor = None\n",
        "model = None\n",
        "model_loaded = False\n",
        "device = None\n",
        "dtype = None\n",
        "\n",
        "# Global variables for CLIP model and processor\n",
        "clip_model = None\n",
        "clip_processor = None\n",
        "clip_model_loaded = False\n"
      ],
      "metadata": {
        "id": "FoWpBS-s6I7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_llava_model():\n",
        "    \"\"\"\n",
        "    Load the LLaVA model and processor from Hugging Face\n",
        "    Model: llava-hf/llava-1.5-7b-hf\n",
        "    \"\"\"\n",
        "    global processor, model, model_loaded, device, dtype\n",
        "\n",
        "    if model_loaded:\n",
        "        print(\"Model already loaded!\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Loading LLaVA model and processor...\")\n",
        "    print(\"This may take 2-5 minutes on first run...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        dtype = torch.float16\n",
        "        print(\"‚úì CUDA available - using GPU\")\n",
        "        print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        dtype = torch.float32\n",
        "        print(\"‚ö†Ô∏è  CUDA not available - using CPU\")\n",
        "        print(\"‚ö†Ô∏è  Note: CPU inference will be slower\")\n",
        "\n",
        "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "    # Load processor\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "    # Load model with appropriate settings for CPU or GPU\n",
        "    if device.type == \"cuda\":\n",
        "        model = LlavaForConditionalGeneration.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    else:\n",
        "        # For CPU, load without device_map and move manually\n",
        "        model = LlavaForConditionalGeneration.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        model = model.to(device)\n",
        "\n",
        "    model_loaded = True\n",
        "\n",
        "    print(\"‚úì Model loaded successfully!\")\n",
        "    print(f\"‚úì Model device: {model.device}\")\n",
        "    print(f\"‚úì Model dtype: {model.dtype}\")\n",
        "    print(\"=\" * 70)\n"
      ],
      "metadata": {
        "id": "Sv03MrPT6L0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clip_model():\n",
        "    \"\"\"\n",
        "    Load the CLIP model (ViT-B/32)\n",
        "    This model acts as the OpenVision encoder\n",
        "    It converts images into 512-dimensional embeddings\n",
        "    \"\"\"\n",
        "    global clip_model, clip_processor, clip_model_loaded\n",
        "\n",
        "    if clip_model_loaded:\n",
        "        print(\"CLIP model already loaded!\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Loading CLIP model and processor...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "    clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "    clip_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Move to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        clip_model = clip_model.to(\"cuda\")\n",
        "        print(\"‚úì CLIP model loaded on GPU\")\n",
        "    else:\n",
        "        print(\"‚úì CLIP model loaded on CPU\")\n",
        "\n",
        "    # Set model to evaluation mode (important: no training)\n",
        "    clip_model.eval()\n",
        "\n",
        "    clip_model_loaded = True\n",
        "    print(\"‚úì CLIP model loaded successfully\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "jtJ00vqcgkv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(image: Image.Image):\n",
        "    \"\"\"\n",
        "    Preprocess image and convert it to model input\n",
        "    Generate normalized embeddings for the image\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image object\n",
        "\n",
        "    Returns:\n",
        "        Normalized embedding tensor of shape [1, 512]\n",
        "    \"\"\"\n",
        "    if not clip_model_loaded:\n",
        "        raise HTTPException(status_code=503, detail=\"CLIP model not loaded yet\")\n",
        "\n",
        "    # Preprocess image\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # Move to same device as model\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    # Disable gradient calculation (we are not training)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(**inputs)\n",
        "\n",
        "    # Normalize embeddings to unit length (L2 normalization)\n",
        "    # This is required for cosine similarity to work correctly\n",
        "    image_embedding = F.normalize(image_features, p=2, dim=1)\n",
        "\n",
        "    return image_embedding"
      ],
      "metadata": {
        "id": "Ufd0eUX9gfR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(embedding1, embedding2):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between two embeddings\n",
        "\n",
        "    Args:\n",
        "        embedding1: First embedding tensor [1, 512]\n",
        "        embedding2: Second embedding tensor [1, 512]\n",
        "\n",
        "    Returns:\n",
        "        Cosine similarity score (float between -1 and 1)\n",
        "    \"\"\"\n",
        "    cosine = torch.nn.CosineSimilarity(dim=1)\n",
        "    similarity = cosine(embedding1, embedding2)\n",
        "    return similarity.item()"
      ],
      "metadata": {
        "id": "gq2wwRCDgcfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_response(image: Image.Image, prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response from LLaVA given an image and text prompt\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image object\n",
        "        prompt: Text prompt/instruction\n",
        "\n",
        "    Returns:\n",
        "        Generated text response\n",
        "    \"\"\"\n",
        "    if not model_loaded:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded yet\")\n",
        "\n",
        "    # Prepare conversation format (LLaVA uses a specific format)\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt_text = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "    # Process inputs\n",
        "    inputs = processor(images=image, text=prompt_text, return_tensors=\"pt\").to(0, torch.float16)\n",
        "\n",
        "    # Generate response\n",
        "    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
        "\n",
        "    # Decode and extract only the generated text (remove prompt)\n",
        "    generated_text = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the assistant's response\n",
        "    if \"ASSISTANT:\" in generated_text:\n",
        "        response = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "    else:\n",
        "        response = generated_text.strip()\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "jlAic_lP6N11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint\"\"\"\n",
        "    return {\n",
        "        \"message\": \"LLaVA Vision-Language Model API\",\n",
        "        \"status\": \"running\",\n",
        "        \"model_loaded\": model_loaded,\n",
        "        \"endpoints\": {\n",
        "            \"analyze\": \"/v1/analyze\",\n",
        "            \"embed\": \"/v1/embed\",\n",
        "            \"cosine_sim\": \"/v1/cosine-sim\",\n",
        "            \"health\": \"/health\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model_loaded\": model_loaded,\n",
        "        \"cuda_available\": torch.cuda.is_available(),\n",
        "        \"device\": str(model.device) if model_loaded else \"N/A\"\n",
        "    }\n",
        "\n",
        "@app.post(\"/v1/analyze\")\n",
        "async def analyze_image(\n",
        "    file: UploadFile = File(...),\n",
        "    prompt: str = Form(...)\n",
        "):\n",
        "    \"\"\"\n",
        "    Analyze an uploaded image using LLaVA model\n",
        "\n",
        "    Args:\n",
        "        file: Image file to analyze\n",
        "        prompt: Text prompt/instruction for the model\n",
        "\n",
        "    Returns:\n",
        "        JSON response with analysis results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate file type\n",
        "        if not file.content_type.startswith(\"image/\"):\n",
        "            raise HTTPException(status_code=400, detail=\"File must be an image\")\n",
        "\n",
        "        # Read and process image\n",
        "        image_bytes = await file.read()\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        print(f\"Processing image: {file.filename}\")\n",
        "        print(f\"Image size: {image.size}\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        # Generate response using LLaVA\n",
        "        response = generate_response(image, prompt)\n",
        "\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"filename\": file.filename,\n",
        "            \"image_size\": {\n",
        "                \"width\": image.size[0],\n",
        "                \"height\": image.size[1]\n",
        "            },\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing image: {str(e)}\")\n",
        "\n",
        "@app.post(\"/v1/embed\")\n",
        "async def embed_image(\n",
        "    file: UploadFile = File(...)\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate embeddings for an uploaded image using CLIP model\n",
        "\n",
        "    Args:\n",
        "        file: Image file to generate embeddings for\n",
        "\n",
        "    Returns:\n",
        "        JSON response with embedding vector (512-dimensional)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate file type\n",
        "        if not file.content_type.startswith(\"image/\"):\n",
        "            raise HTTPException(status_code=400, detail=\"File must be an image\")\n",
        "\n",
        "        # Read and process image\n",
        "        image_bytes = await file.read()\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        print(f\"Generating embeddings for: {file.filename}\")\n",
        "        print(f\"Image size: {image.size}\")\n",
        "\n",
        "        # Generate embeddings using CLIP\n",
        "        embedding = generate_embeddings(image)\n",
        "\n",
        "        # Convert to list for JSON serialization\n",
        "        embedding_list = embedding.cpu().numpy().tolist()[0]\n",
        "\n",
        "        print(f\"Embedding shape: {embedding.shape}\")\n",
        "        print(f\"Embedding generated successfully\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"filename\": file.filename,\n",
        "            \"image_size\": {\n",
        "                \"width\": image.size[0],\n",
        "                \"height\": image.size[1]\n",
        "            },\n",
        "            \"embedding\": embedding_list,\n",
        "            \"embedding_shape\": list(embedding.shape),\n",
        "            \"embedding_dim\": embedding.shape[1]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embeddings: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error generating embeddings: {str(e)}\")\n",
        "\n",
        "@app.post(\"/v1/cosine-sim\")\n",
        "async def calculate_cosine_similarity(\n",
        "    file1: UploadFile = File(...),\n",
        "    file2: UploadFile = File(...)\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between two uploaded images using CLIP embeddings\n",
        "\n",
        "    Args:\n",
        "        file1: First image file\n",
        "        file2: Second image file\n",
        "\n",
        "    Returns:\n",
        "        JSON response with cosine similarity score (range: -1 to 1)\n",
        "        - 1.0 means identical images\n",
        "        - 0.0 means orthogonal/unrelated\n",
        "        - -1.0 means opposite (rare in practice)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate file types\n",
        "        if not file1.content_type.startswith(\"image/\"):\n",
        "            raise HTTPException(status_code=400, detail=\"File1 must be an image\")\n",
        "        if not file2.content_type.startswith(\"image/\"):\n",
        "            raise HTTPException(status_code=400, detail=\"File2 must be an image\")\n",
        "\n",
        "        # Read and process first image\n",
        "        image1_bytes = await file1.read()\n",
        "        image1 = Image.open(io.BytesIO(image1_bytes)).convert(\"RGB\")\n",
        "\n",
        "        # Read and process second image\n",
        "        image2_bytes = await file2.read()\n",
        "        image2 = Image.open(io.BytesIO(image2_bytes)).convert(\"RGB\")\n",
        "\n",
        "        print(f\"Calculating similarity between: {file1.filename} and {file2.filename}\")\n",
        "        print(f\"Image1 size: {image1.size}\")\n",
        "        print(f\"Image2 size: {image2.size}\")\n",
        "\n",
        "        # Generate embeddings for both images\n",
        "        embedding1 = generate_embeddings(image1)\n",
        "        embedding2 = generate_embeddings(image2)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity_score = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "        print(f\"Cosine similarity: {similarity_score:.4f}\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"file1\": {\n",
        "                \"filename\": file1.filename,\n",
        "                \"size\": {\"width\": image1.size[0], \"height\": image1.size[1]}\n",
        "            },\n",
        "            \"file2\": {\n",
        "                \"filename\": file2.filename,\n",
        "                \"size\": {\"width\": image2.size[0], \"height\": image2.size[1]}\n",
        "            },\n",
        "            \"cosine_similarity\": similarity_score,\n",
        "            \"interpretation\": {\n",
        "                \"score\": similarity_score,\n",
        "                \"description\": (\n",
        "                    \"Very similar\" if similarity_score > 0.9 else\n",
        "                    \"Similar\" if similarity_score > 0.7 else\n",
        "                    \"Somewhat similar\" if similarity_score > 0.5 else\n",
        "                    \"Different\" if similarity_score > 0.3 else\n",
        "                    \"Very different\"\n",
        "                )\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating cosine similarity: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error calculating cosine similarity: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "heUUVvZc6QFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ngrok tunnel and run server\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STARTING LLAVA FASTAPI SERVER\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load models before starting server\n",
        "    load_llava_model()\n",
        "    load_clip_model()\n",
        "\n",
        "    # Set ngrok auth token (you'll need to add your token)\n",
        "    # Get free token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "    # Uncomment and add your token:\n",
        "    ngrok.set_auth_token(\"37lUpwcYVFfOuHkVQBCfhfZ66OE_7uSiArpZk7HsmLcHX3Wqd\")\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    print(\"\\nStarting ngrok tunnel...\")\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"üåê PUBLIC URL: {public_url}\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: Copy this URL and use it in your local app.py\")\n",
        "    print(f\"   Update COLAB_SERVER_URL = '{public_url}'\\n\")\n",
        "    print(f\"{\"=\" * 70}\\n\")\n",
        "\n",
        "    # Run FastAPI server using threading to avoid event loop issues\n",
        "    config = uvicorn.Config(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "\n",
        "    # Run server in a thread to work in Colab\n",
        "    thread = threading.Thread(target=server.run)\n",
        "    thread.start()\n",
        "\n",
        "    print(\"\\n‚úì Server is running!\")\n",
        "    print(\"‚úì Keep this cell running - do NOT stop it\")\n",
        "    print(\"‚úì Use the public URL above in your local application\\n\")\n",
        "\n",
        "    # Keep the main thread alive\n",
        "    thread.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d9719c562f4343808878eb70a742fd35",
            "92ec6fa254b0451f98dafdc46d89f61b",
            "013cef8ecfb0420397c92b591ab24e4e",
            "945d21b9aadc4e67855e6404fd3619d4",
            "c486bbf31bdd4a798e7522d56613dcca",
            "803e045002e84481a3998adb95867b4b",
            "96834b2eb7b74e59a9ed1144f9f7d9ab",
            "5c65f3e2924e4163b51fe8020335a8e8",
            "2804ad5cafa9463ab34d56e12597a304",
            "08d920393404488b83f06b07a0765a44",
            "1dcd16bab8a249d88db2fea0bc81926a"
          ]
        },
        "id": "jAA19JHR6R_b",
        "outputId": "94b0ada1-89f0-4518-9f93-025716b3cd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING LLAVA FASTAPI SERVER\n",
            "======================================================================\n",
            "======================================================================\n",
            "Loading LLaVA model and processor...\n",
            "This may take 2-5 minutes on first run...\n",
            "======================================================================\n",
            "‚úì CUDA available - using GPU\n",
            "‚úì GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9719c562f4343808878eb70a742fd35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model loaded successfully!\n",
            "‚úì Model device: cuda:0\n",
            "‚úì Model dtype: torch.float16\n",
            "======================================================================\n",
            "======================================================================\n",
            "Loading CLIP model and processor...\n",
            "======================================================================\n",
            "‚úì CLIP model loaded on GPU\n",
            "‚úì CLIP model loaded successfully\n",
            "======================================================================\n",
            "\n",
            "Starting ngrok tunnel...\n",
            "\n",
            "======================================================================\n",
            "üåê PUBLIC URL: NgrokTunnel: \"https://kenna-explosible-nonmonistically.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT: Copy this URL and use it in your local app.py\n",
            "   Update COLAB_SERVER_URL = 'NgrokTunnel: \"https://kenna-explosible-nonmonistically.ngrok-free.dev\" -> \"http://localhost:8000\"'\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [4348]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Server is running!\n",
            "‚úì Keep this cell running - do NOT stop it\n",
            "‚úì Use the public URL above in your local application\n",
            "\n",
            "Calculating similarity between: apple_leaf.png and tomato-healthy.png\n",
            "Image1 size: (212, 214)\n",
            "Image2 size: (274, 156)\n",
            "Cosine similarity: 0.7979\n",
            "INFO:     192.116.63.38:0 - \"POST /v1/cosine-sim HTTP/1.1\" 200 OK\n",
            "Processing image: WhatsApp Image 2025-12-27 at 19.16.53.jpeg\n",
            "Image size: (1200, 800)\n",
            "Prompt: Describe this image in one concise sentence.\n",
            "Response: A plant with green leaves and red spots.\n",
            "INFO:     192.116.63.38:0 - \"POST /v1/analyze HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_1aSaBK2Qgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}